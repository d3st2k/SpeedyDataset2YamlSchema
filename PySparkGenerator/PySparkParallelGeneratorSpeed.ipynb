{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Needed packages to run everything\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "import yaml\n",
    "\n",
    "\"\"\"General methods that are dictate what stable version the file is currently in, how the file is read, created and how the metadata is dumped into it\"\"\"\n",
    "\n",
    "class generalMethods:\n",
    "    def __init__(self, majorVersion: int, minorVersion : int) -> None:\n",
    "        \"\"\"\n",
    "        Constructor of the class general methods \n",
    "        ...\n",
    "        Arguments\n",
    "        ----------\n",
    "        self: \n",
    "            variable that calls this class to create an instance\n",
    "        majorVersion : int\n",
    "            current major version that is stable for instance\n",
    "        minorVersion : int\n",
    "            current minor version that is stable for instance\n",
    "        ...\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.majorVersion = majorVersion\n",
    "        self.minorVersion = minorVersion\n",
    "        self.stableVersion = f\"{self.majorVersion}.{self.minorVersion}\"\n",
    "\n",
    "    def writeMetadataFile(self, schema: dict, outputMetadataPath: str, fileName: str, outputFileType : str = \"yaml\") -> None:\n",
    "        \"\"\"\n",
    "        Write generated metadata, with the current stable version\n",
    "        ...\n",
    "        Arguments\n",
    "        ----------\n",
    "        schema : dict\n",
    "            dictionary that contains all the data that needs to be inputed inside the .yaml file to structure the metadata\n",
    "        outputMetadataPath : str\n",
    "            path of where the metadata files are going to be saved, so that they are structured and easy to find\n",
    "        fileName : str\n",
    "            path of where each folder is gonna be grouped based on the type of the file\n",
    "        stableVersion : str\n",
    "            latest stable version that the metadata will be generated, which is depended on the global variables majorVersion and minorVersion\n",
    "        outputFileType : str\n",
    "            output file extension (type) that we want to create, default value is `yaml` if no value is given for that parameter\n",
    "        ...\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        with open(f'{outputMetadataPath}/{fileName}/{fileName}%{self.stableVersion}.{outputFileType}', 'w') as file:\n",
    "            yaml.dump(schema, file, sort_keys=False, version=(self.majorVersion, self.minorVersion))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import threading\n",
    "lock = threading.Lock()\n",
    "\n",
    "\"\"\"Default functions to check if certain values fulfill certain criteria and functions that manipulate with the global \"sources\" variable\"\"\"\n",
    "\n",
    "\"\"\"Mapping string value to boolean so we can use where ever they used\"\"\"\n",
    "strBoolMapping = {\n",
    "    \"y\": True,\n",
    "    \"n\": False,\n",
    "    \"gdpr\": \"true\"\n",
    "}\n",
    "\n",
    "def isNull(nullable : str) -> bool:\n",
    "    \"\"\"\n",
    "    Finding from the dataframe column if that column can have null values (is it required or not)\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    nullable : str\n",
    "        cell value of the row that we are currently checking, on the column \"NULLABLE\"\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    Returns a boolean value, based on the variable strBoolMapping\n",
    "    \"\"\"\n",
    "    return strBoolMapping[nullable.lower()]\n",
    "\n",
    "def isDate(dataType : str) -> bool:\n",
    "    \"\"\"\n",
    "    Function returns extra metadata for column \"DATA_TYPE\", if that column fulfills a certain condition (is a date type)\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    dataType : str\n",
    "        cell value of the row that we are currently checking, on the column \"DATA_TYPE\"\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    Retuns a boolean value, if that specific value has type date\n",
    "    \"\"\"\n",
    "    dateTypeChecked = \"date\"\n",
    "    dateBool = dataType.lower() == dateTypeChecked\n",
    "    return dateBool\n",
    "\n",
    "def isGdpr(gdprFlag : str) -> bool:\n",
    "    \"\"\"\n",
    "    Function returns extra metadata for column \"GDPR_FLAG\", if that row fulfills a certain condition (has the cell value \"true\")\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    gdprFlag : str\n",
    "        cell value of the row that we are currently checking, on the column \"GDPR_FLAG\"\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    Boolean value, if the value that we want to compare is equal that to the value that is being compared by \n",
    "    \"\"\"\n",
    "    gdprBool = gdprFlag.lower() == strBoolMapping[\"gdpr\"]\n",
    "    return gdprBool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Needed packages to run everything in this file\"\"\"\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\"\"\"Functions that generate all the metadata for different parts of a dataframe \"\"\"\n",
    "    \n",
    "def genPrimaryArgsMeta(title : str, sourceSystem : str, description : str = \"A metaschema generated from an example dataset.\") -> dict:\n",
    "    \"\"\"\n",
    "    Write general info of metadata file\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    title : str\n",
    "        variable that is gonna be used for the title of the metadata file\n",
    "    sourceSystem : \n",
    "    description : str\n",
    "        variable that is gonna be used for the description of the metadata file, also has a default value which is gonna be used when there is not value give\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    It returns a dictionary of the values mentioned above which are gonna be used later to build the entires metadata file\n",
    "    \"\"\"\n",
    "    version = 2\n",
    "    primaryArgSchema = {\n",
    "        \"version\": version,\n",
    "        \"source_system\": sourceSystem,\n",
    "        \"title\": title,\n",
    "        \"description\": f\"{description}\",\n",
    "        \"tables\": []\n",
    "    }\n",
    "    return primaryArgSchema\n",
    "    \n",
    "def genTableArgsMeta(tableName : str, sourceRef : int, inputSchema : list[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Generating primary attributes for metadata schema\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    tableName : str\n",
    "        name of the table that we are generating metadata for\n",
    "    sourceSystem : str\n",
    "        name of the source system that the table column is part of\n",
    "    inputSchema : list[dict]\n",
    "        list of dictionaries that are placed in the `columns` attribute\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionary of the primary metadata attributes for the table that we are checking \n",
    "    \"\"\"\n",
    "    tableArgMeta = {\n",
    "        \"name\": f\"{tableName}\",\n",
    "        \"source\": sourceRef,\n",
    "        \"columns\": inputSchema,\n",
    "        \"primary_key\": \"unid\",\n",
    "        \"cdc_column\": \"dml_flag\",\n",
    "        \"cdc_type\": \"soft\" \n",
    "    }\n",
    "    return tableArgMeta\n",
    "\n",
    "def genTableArgsGdpr(tableName : str, inputSchema : list[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Generating primary attributes for gdpr schema\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    tableName : str\n",
    "        name of the table that we are generating metadata for\n",
    "    inputSchema : list[dict]\n",
    "        list of dictionaries that are placed in the `personal < hash` attribute\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionary of the primary gdpr attributes for the table that we are checking \n",
    "    \"\"\"\n",
    "    tableArgGdpr = {\n",
    "        \"name\": tableName,\n",
    "        \"personal_data\": {\"hash\" : inputSchema}\n",
    "    }\n",
    "    return tableArgGdpr\n",
    "\n",
    "def genColumnSchema(tableDataframe : DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Generating the schema metadata for each column on a table\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    tableDataframe : DataFrame\n",
    "        a piece of the original dataframe that has the column \"table_name\" equal to a specific table name that enables us to see all the columns in that specific piece of dataframe\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    Function returns a dictionary that contains metadata for each dataframe column (like: name, data_type, is_nullable, date_format etc)\n",
    "    \"\"\"\n",
    "    columnSchema = {\n",
    "        \"columns\": [],\n",
    "        \"hash\": []\n",
    "    }\n",
    "    for row in tableDataframe:\n",
    "        columnName = row[5].lower()\n",
    "        dataType = row[6].lower()\n",
    "        if \"number\" in dataType:\n",
    "            dataType = dataType.replace(\"number\", \"decimal\")\n",
    "        if \"varchar\" in dataType:\n",
    "            dataType = \"string\"\n",
    "        column_info = {\n",
    "            \"name\": columnName,\n",
    "            \"data_type\": dataType\n",
    "        }\n",
    "        if isNull(row[10]):\n",
    "            column_info[\"is_nullable\"] = True\n",
    "        if isDate(dataType):\n",
    "            column_info[\"date_format\"] = \"yyyy-MM-dd HH:mm:ss\"\n",
    "        if isGdpr(row[14]):\n",
    "            columnSchema[\"hash\"].append(columnName)\n",
    "        columnSchema[\"columns\"].append(column_info)\n",
    "    return columnSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Needed packages to run everything\"\"\"\n",
    "from pyspark.sql import DataFrame\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\"\"\"All the functions to generate the schemas are called here and that data is saved in the variable \"schema\\\"\"\"\"\n",
    "\n",
    "def datasetTableInfo(dataFrame : DataFrame, metadataTitle : str, description : str = \"A metaschema generated from an example dataset.\") -> dict:\n",
    "    \"\"\"\n",
    "    All the pieces of the whole schema are generated here and are merged together\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    dataFrame : DataFrame\n",
    "        the dataframe that we are gonna generate all the schema for\n",
    "    metadataTitle : str\n",
    "        title of the schema file that we are gonna call it\n",
    "    description : str\n",
    "        description about the schema, if this argument is not sent the default value is used\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    The whole generated schema about this dataframe\n",
    "    \"\"\"\n",
    "    firstRow = dataFrame.head(1)[0]\n",
    "    source = {\n",
    "        \"load_type\": \"delta\",\n",
    "        \"file_pattern\": firstRow.FILE_NAME.lower(),\n",
    "        \"params\": {\n",
    "            \"decimal_separator\": firstRow.DECIMAL_SEPARATOR,\n",
    "            \"format\": firstRow.FILE_TYPE.lower()\n",
    "        }\n",
    "    }\n",
    "    sourceSystem = firstRow.SOURCE_SYSTEM\n",
    "    completeSchema = {\n",
    "        \"metadata\": genPrimaryArgsMeta(metadataTitle, sourceSystem, description),\n",
    "        \"gdpr\": []\n",
    "    }\n",
    "    separatedDataFrames = {}\n",
    "    for row in dataFrame.collect():\n",
    "        table_name = row[\"table_name\"]\n",
    "        row_values = list(row.asDict().values())\n",
    "        if table_name in separatedDataFrames:\n",
    "            separatedDataFrames[table_name].append(row_values)\n",
    "        else:\n",
    "            separatedDataFrames[table_name] = [row_values]\n",
    "            \n",
    "    def processDataframe(key_value):\n",
    "        key, value = key_value\n",
    "        sourceRef = source\n",
    "        columnSchema = genColumnSchema(value)\n",
    "        tableArgsMeta = genTableArgsMeta(key, sourceRef, columnSchema[\"columns\"])\n",
    "        tableArgsGdpr = genTableArgsGdpr(key, columnSchema[\"hash\"])\n",
    "        return tableArgsMeta, tableArgsGdpr\n",
    "\n",
    "    def parallelProcessDataframes(dataframes):\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            results = executor.map(processDataframe, dataframes.items())\n",
    "            for tableArgsMeta, tableArgsGdpr in results:\n",
    "                completeSchema[\"metadata\"][\"tables\"].append(tableArgsMeta)\n",
    "                completeSchema[\"gdpr\"].append(tableArgsGdpr)\n",
    "        return completeSchema\n",
    "    \n",
    "    return parallelProcessDataframes(separatedDataFrames)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4472897052764893, 2.6291465759277344, 2.5403432846069336, 2.7508511543273926, 2.6073222160339355, 2.669354200363159, 2.431424856185913, 2.587740659713745, 2.580059766769409, 2.5330612659454346]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"Basic variables that define current stable version of schemas, \"SparkSession\" object, paths of where to read and save files and calling functions that generate and dump the schema to those files\"\"\"\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\"\"\"Gloabl varibles for major and minor stable metadata and gdpr versions\"\"\"\n",
    "majorMetadataVersion = 1\n",
    "minorMetadataVersion = 6\n",
    "metadata = generalMethods(majorMetadataVersion, minorMetadataVersion)\n",
    "\n",
    "majorGdprVersion = 1\n",
    "minorGdprVersion = 6\n",
    "stableGdprVersion = generalMethods(majorGdprVersion, minorGdprVersion)\n",
    "\n",
    "\"\"\"Global input metadata path\"\"\"\n",
    "inputDataFramePath = \"/app/syntheticDataset100K\"\n",
    "\n",
    "\"\"\"Global output metadata path\"\"\"\n",
    "outputSchemaPath = \"../Output Metadata/Schema Versions\"\n",
    "metadataFileName = \"MetadataSpeedTest\"\n",
    "gdprFileName = \"GdprSpeedTest\"\n",
    "\n",
    "\n",
    "\"\"\"Saving the dataframe into a variable so that we don't have to call the function readCSV() each time!\"\"\"\n",
    "dataFrame = spark.read.csv(f\"{inputDataFramePath}.csv\", header=True, sep=';').cache()\n",
    "\n",
    "\"\"\"The variable where we save the time each iteration takes\"\"\"\n",
    "time_list = []\n",
    "\n",
    "\"\"\"Iterate 10 times over the same task, so we can get the average time it takes over 10 tests\"\"\"\n",
    "for _ in range(10):\n",
    "    \"\"\"Start the timer, not including the time it takes to read the csv\"\"\"\n",
    "    beginTime = time.time()\n",
    "\n",
    "    \"\"\"Generating the metadata from the dataset and saving it into a variable\"\"\"\n",
    "    primaryArgSchema = datasetTableInfo(dataFrame, \"TestMetadata\")\n",
    "    metadataSchema = primaryArgSchema[\"metadata\"]\n",
    "    gdprSchema = primaryArgSchema[\"gdpr\"]\n",
    "\n",
    "    \"\"\"Stop the time after also the spark session has stoped\"\"\"\n",
    "    timeElapsed = time.time() - beginTime\n",
    "\n",
    "    \"\"\"Append the time to the overall time consumed for each iteration\"\"\"\n",
    "    time_list.append(timeElapsed)\n",
    "\n",
    "\"\"\"Show how much each iteration has taken\"\"\"\n",
    "print(time_list)\n",
    "\n",
    "def save_time_speeds(file_path: str, time_list: list, algorithm: str, dataset: str) -> None:\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(\"\\n\")\n",
    "        file.write(f\"Algorithm: {algorithm}\\n\")\n",
    "        file.write(f\"Dataset: {dataset}\\n\")\n",
    "        file.write(\"Time speeds (in seconds):\\n\")\n",
    "        for time_elapsed in time_list:\n",
    "            file.write(f\"{time_elapsed}\\n\")\n",
    "\n",
    "algorithm = \"Parallel PySpark Speed\"\n",
    "dataset = \"100000 rows, 1000 tables\"\n",
    "save_time_speeds('../timeSpeeds.txt', time_list, algorithm, dataset)\n",
    "\n",
    "\"\"\"Using generated metadata, output paths and the current latest stable version to dump the metadata into a .yaml schema \"\"\"\n",
    "# metadata.writeMetadataFile(metadataSchema, outputSchemaPath, metadataFileName)\n",
    "# metadata.writeMetadataFile(gdprSchema, outputSchemaPath, gdprFileName)\n",
    "\n",
    "\"\"\"Stop the spark session\"\"\"\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
